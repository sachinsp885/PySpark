{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f09df18",
   "metadata": {},
   "source": [
    "topics I  covered\n",
    "->file_reading\n",
    "->schema\n",
    "->select\n",
    "->alias\n",
    "->where_filter\n",
    "->withcolumn and withcolumnrenamed\n",
    "->Typecasting\n",
    "->sort and order_by\n",
    "->limit\n",
    "->drop\n",
    "->drop_duplicates\n",
    "->union and union_by\n",
    "->string_funtion\n",
    "->date_func\n",
    "->handling_null\n",
    "->split and indexing\n",
    "->explode\n",
    "->array_contains\n",
    "->group_by\n",
    "->collect_list\n",
    "->pivot\n",
    "->when and otherwise\n",
    "->joins\n",
    "->window function\n",
    "->udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285ce1e",
   "metadata": {},
   "source": [
    "##### File reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c58446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+---------------+-----------------+--------+------------+----------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer Id|First Name|Last Name|        Company|             City| Country|     Phone 1|         Phone 2|               Email|Subscription Date|             Website|\n",
      "+-----+---------------+----------+---------+---------------+-----------------+--------+------------+----------------+--------------------+-----------------+--------------------+\n",
      "|    1|DD37Cf93aecA6Dc|    Sheryl|   Baxter|Rasmussen Group|     East Leonard|   Chile|229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    2|1Ef7b82A4CAAD10|   Preston|   Lozano|    Vega-Gentry|East Jimmychester|Djibouti|  5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "+-----+---------------+----------+---------+---------------+-----------------+--------+------------+----------------+--------------------+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "               .option(\"inferSchema\",True)\\\n",
    "               .option(\"header\",True)\\\n",
    "               .load(\"C:/Git files/My git files/PySpark/files/test.csv\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216050f3",
   "metadata": {},
   "source": [
    "#### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56d4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+--------+-------+\n",
      "|amount|customer|      date| product|sale_id|\n",
      "+------+--------+----------+--------+-------+\n",
      "| 56000|    Arun|2025-01-03|  Laptop|      1|\n",
      "|   800|   Meena|2025-01-04|   Mouse|      2|\n",
      "|  1500|    John|2025-01-05|Keyboard|      3|\n",
      "|  7200|   Priya|2025-01-06| Monitor|      4|\n",
      "+------+--------+----------+--------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1,  \"date\": \"2025-01-03\", \"customer\": \"Arun\",     \"product\": \"Laptop\",      \"amount\": 56000},\n",
    "    {\"sale_id\": 2,  \"date\": \"2025-01-04\", \"customer\": \"Meena\",    \"product\": \"Mouse\",       \"amount\": 800},\n",
    "    {\"sale_id\": 3,  \"date\": \"2025-01-05\", \"customer\": \"John\",     \"product\": \"Keyboard\",    \"amount\": 1500},\n",
    "    {\"sale_id\": 4,  \"date\": \"2025-01-06\", \"customer\": \"Priya\",    \"product\": \"Monitor\",     \"amount\": 7200},\n",
    "    {\"sale_id\": 5,  \"date\": \"2025-01-07\", \"customer\": \"Sneha\",    \"product\": \"Laptop\",      \"amount\": 53000},\n",
    "    {\"sale_id\": 6,  \"date\": \"2025-01-08\", \"customer\": \"Kiran\",    \"product\": \"Tablet\",      \"amount\": 18000},\n",
    "    {\"sale_id\": 7,  \"date\": \"2025-01-09\", \"customer\": \"Rahul\",    \"product\": \"Earphones\",   \"amount\": 1200},\n",
    "    {\"sale_id\": 8,  \"date\": \"2025-01-10\", \"customer\": \"Vijay\",    \"product\": \"Speaker\",     \"amount\": 2500},\n",
    "    {\"sale_id\": 9,  \"date\": \"2025-01-11\", \"customer\": \"Anjali\",   \"product\": \"Webcam\",      \"amount\": 3000},\n",
    "    {\"sale_id\": 10, \"date\": \"2025-01-12\", \"customer\": \"Nitin\",    \"product\": \"Printer\",     \"amount\": 9000},\n",
    "    {\"sale_id\": 11, \"date\": \"2025-01-13\", \"customer\": \"Divya\",    \"product\": \"Laptop\",      \"amount\": 59000},\n",
    "    {\"sale_id\": 12, \"date\": \"2025-01-14\", \"customer\": \"Harish\",   \"product\": \"Mouse\",       \"amount\": 850},\n",
    "    {\"sale_id\": 13, \"date\": \"2025-01-15\", \"customer\": \"Suman\",    \"product\": \"Keyboard\",    \"amount\": 1600},\n",
    "    {\"sale_id\": 14, \"date\": \"2025-01-16\", \"customer\": \"Geeta\",    \"product\": \"Monitor\",     \"amount\": 7100},\n",
    "    {\"sale_id\": 15, \"date\": \"2025-01-17\", \"customer\": \"Mahesh\",   \"product\": \"Tablet\",      \"amount\": 17500},\n",
    "    {\"sale_id\": 16, \"date\": \"2025-01-18\", \"customer\": \"Asha\",     \"product\": \"Speaker\",     \"amount\": 2600},\n",
    "    {\"sale_id\": 17, \"date\": \"2025-01-19\", \"customer\": \"Lokesh\",   \"product\": \"Laptop\",      \"amount\": 60000},\n",
    "    {\"sale_id\": 18, \"date\": \"2025-01-20\", \"customer\": \"Tarun\",    \"product\": \"Webcam\",      \"amount\": 2800},\n",
    "    {\"sale_id\": 19, \"date\": \"2025-01-21\", \"customer\": \"Gauri\",    \"product\": \"Earphones\",   \"amount\": 1100},\n",
    "    {\"sale_id\": 20, \"date\": \"2025-01-22\", \"customer\": \"Sathish\",  \"product\": \"Printer\",     \"amount\": 9500}\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data=sales_data)\n",
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14eca58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Struct_schema\n",
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- customer: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n",
      "ddl_scehma\n",
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- customer: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "my_struct_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "    \n",
    "])\n",
    "print(\"Struct_schema\")\n",
    "df2 = spark.read.format('csv')\\\n",
    "                .schema(my_struct_schema)\\\n",
    "                .option('header',True)\\\n",
    "                .load(\"C:/Git files/My git files/PySpark/files/sales.csv\")\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "my_ddl_schema = \"\"\"\n",
    "sale_id INT,\n",
    "date STRING,\n",
    "customer STRING,\n",
    "product STRING,\n",
    "amount DOUBLE\n",
    "\"\"\"\n",
    "\n",
    "print(\"ddl_scehma\")\n",
    "df1 = spark.read.format('csv')\\\n",
    "                .schema(my_ddl_schema)\\\n",
    "                .option('header',True)\\\n",
    "                .load(\"C:/Git files/My git files/PySpark/files/sales.csv\")\n",
    "\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1b060",
   "metadata": {},
   "source": [
    "#### select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8d5cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|customer| amount|\n",
      "+--------+-------+\n",
      "|    Arun|56000.0|\n",
      "|   Meena|  800.0|\n",
      "|    John| 1500.0|\n",
      "+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('customer','amount').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563903f",
   "metadata": {},
   "source": [
    "#### alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d98fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|customer_name|\n",
      "+-------------+\n",
      "|         Arun|\n",
      "|        Meena|\n",
      "|         John|\n",
      "|        Priya|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(col(\"customer\").alias(\"customer_name\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121b2eb",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42df87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+-------+-------+\n",
      "|sale_id|      date|customer|product| amount|\n",
      "+-------+----------+--------+-------+-------+\n",
      "|      1|2025-01-03|    Arun| Laptop|56000.0|\n",
      "|      9|2025-01-11|  Anjali| Webcam| 3000.0|\n",
      "|     16|2025-01-18|    Asha|Speaker| 2600.0|\n",
      "+-------+----------+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(col(\"customer\").startswith('A')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590503d",
   "metadata": {},
   "source": [
    "#### withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5680f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------+-------+\n",
      "|sale_id|      date|customer_name| product| amount|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "|      1|2025-01-03|         Arun|  Laptop|56000.0|\n",
      "|      2|2025-01-04|        Meena|   Mouse|  800.0|\n",
      "|      3|2025-01-05|         John|Keyboard| 1500.0|\n",
      "|      4|2025-01-06|        Priya| Monitor| 7200.0|\n",
      "|      5|2025-01-07|        Sneha|  Laptop|53000.0|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumnRenamed(\"customer\",\"customer_name\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe9dab",
   "metadata": {},
   "source": [
    "#### Typecasting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3df5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumn(\"date\",col(\"date\").cast(DateType()))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe403b1",
   "metadata": {},
   "source": [
    "#### sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86268bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------+-------+\n",
      "|sale_id|      date|customer_name| product| amount|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "|      8|2025-01-10|        Vijay| Speaker| 2500.0|\n",
      "|     18|2025-01-20|        Tarun|  Webcam| 2800.0|\n",
      "|     13|2025-01-15|        Suman|Keyboard| 1600.0|\n",
      "|      5|2025-01-07|        Sneha|  Laptop|53000.0|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------+----------+-------------+---------+-------+\n",
      "|sale_id|      date|customer_name|  product| amount|\n",
      "+-------+----------+-------------+---------+-------+\n",
      "|     20|2025-01-22|      Sathish|  Printer| 9500.0|\n",
      "|     19|2025-01-21|        Gauri|Earphones| 1100.0|\n",
      "|     18|2025-01-20|        Tarun|   Webcam| 2800.0|\n",
      "|     17|2025-01-19|       Lokesh|   Laptop|60000.0|\n",
      "+-------+----------+-------------+---------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[sale_id: int, date: date, customer_name: string, product: string, amount: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sort(col(\"customer_name\").desc()).show(4)\n",
    "df1.sort([\"sale_id\",\"customer_name\"],ascending=[0,1]).show(4)\n",
    "df1.sort(col('sale_id').asc(),col('customer_name').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a259f",
   "metadata": {},
   "source": [
    "#### limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05b2954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------+-------+\n",
      "|sale_id|      date|customer_name| product| amount|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "|      1|2025-01-03|         Arun|  Laptop|56000.0|\n",
      "|      2|2025-01-04|        Meena|   Mouse|  800.0|\n",
      "|      3|2025-01-05|         John|Keyboard| 1500.0|\n",
      "|      4|2025-01-06|        Priya| Monitor| 7200.0|\n",
      "|      5|2025-01-07|        Sneha|  Laptop|53000.0|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a24fa",
   "metadata": {},
   "source": [
    "#### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91697fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+-------+\n",
      "|sale_id|      date|customer_name| amount|\n",
      "+-------+----------+-------------+-------+\n",
      "|      1|2025-01-03|         Arun|56000.0|\n",
      "|      2|2025-01-04|        Meena|  800.0|\n",
      "|      3|2025-01-05|         John| 1500.0|\n",
      "|      4|2025-01-06|        Priya| 7200.0|\n",
      "+-------+----------+-------------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------+----------+-------------+\n",
      "|sale_id|      date|customer_name|\n",
      "+-------+----------+-------------+\n",
      "|      1|2025-01-03|         Arun|\n",
      "|      2|2025-01-04|        Meena|\n",
      "|      3|2025-01-05|         John|\n",
      "|      4|2025-01-06|        Priya|\n",
      "+-------+----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropiing column name from dataframe\n",
    "df1.drop(col(\"product\")).show(4)\n",
    "df1.drop(\"product\",\"amount\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f55c93",
   "metadata": {},
   "source": [
    "#### drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f9a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+---------+-------+\n",
      "|sale_id|      date|customer_name|  product| amount|\n",
      "+-------+----------+-------------+---------+-------+\n",
      "|      5|2025-01-07|        Sneha|   Laptop|53000.0|\n",
      "|     17|2025-01-19|       Lokesh|   Laptop|60000.0|\n",
      "|      7|2025-01-09|        Rahul|Earphones| 1200.0|\n",
      "|      3|2025-01-05|         John| Keyboard| 1500.0|\n",
      "+-------+----------+-------------+---------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------+----------+-------------+--------+-------+\n",
      "|sale_id|      date|customer_name| product| amount|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "|     20|2025-01-22|      Sathish| Printer| 9500.0|\n",
      "|     11|2025-01-13|        Divya|  Laptop|59000.0|\n",
      "|      2|2025-01-04|        Meena|   Mouse|  800.0|\n",
      "|     13|2025-01-15|        Suman|Keyboard| 1600.0|\n",
      "+-------+----------+-------------+--------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#droping duplicates from dataframe\n",
    "df1.dropDuplicates().show(4)\n",
    "df1.dropDuplicates(subset=[\"customer_name\",\"product\"]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea184c31",
   "metadata": {},
   "source": [
    "#### union or unionByName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18af0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+--------+-------+\n",
      "|amount|customer|      date| product|sale_id|\n",
      "+------+--------+----------+--------+-------+\n",
      "| 56000|    Arun|2025-01-03|  Laptop|      1|\n",
      "|   800|   Meena|2025-01-04|   Mouse|      2|\n",
      "|  1500|    John|2025-01-05|Keyboard|      3|\n",
      "|  7200|   Priya|2025-01-06| Monitor|      4|\n",
      "| 53000|   Sneha|2025-01-07|  Laptop|      5|\n",
      "+------+--------+----------+--------+-------+\n",
      "\n",
      "+------+--------+----------+--------+-------+\n",
      "|amount|customer|      date| product|sale_id|\n",
      "+------+--------+----------+--------+-------+\n",
      "| 56000|    Arun|2025-01-03|  Laptop|      1|\n",
      "|   800|   Meena|2025-01-04|   Mouse|      2|\n",
      "|  1500|    John|2025-01-05|Keyboard|      3|\n",
      "|  7200|   Priya|2025-01-06| Monitor|      4|\n",
      "| 53000|   Sneha|2025-01-07|  Laptop|      5|\n",
      "+------+--------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "        {\"sale_id\": 1, \"date\": \"2025-01-03\", \"customer\": \"Arun\", \"product\": \"Laptop\", \"amount\": 56000}, \n",
    "        {\"sale_id\": 2, \"date\": \"2025-01-04\", \"customer\": \"Meena\", \"product\": \"Mouse\", \"amount\": 800}\n",
    "        ]\n",
    "\n",
    "data2 =[\n",
    "    {\"sale_id\": 3, \"date\": \"2025-01-05\", \"customer\": \"John\", \"product\": \"Keyboard\", \"amount\": 1500}, \n",
    "    {\"sale_id\": 4, \"date\": \"2025-01-06\", \"customer\": \"Priya\", \"product\": \"Monitor\", \"amount\": 7200}, \n",
    "    {\"sale_id\": 5, \"date\": \"2025-01-07\", \"customer\": \"Sneha\", \"product\": \"Laptop\", \"amount\": 53000}\n",
    "]\n",
    "\n",
    "df_data1 = spark.createDataFrame(data=data1)\n",
    "df_data2 = spark.createDataFrame(data=data2)\n",
    "\n",
    "df= df_data1.union(df_data2)\n",
    "df.show()\n",
    "\n",
    "df= df_data1.unionByName(df_data2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976eed0",
   "metadata": {},
   "source": [
    "#### Srting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ae82f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------+-------+---------------+------------+---------------+------------+\n",
      "|sale_id|      date|customer_name| product| amount|      full_name|   trim_name|     ltrim_name|  rtrim_name|\n",
      "+-------+----------+-------------+--------+-------+---------------+------------+---------------+------------+\n",
      "|      1|2025-01-03|         Arun|  Laptop|56000.0| Arun    AAA   | Arun    AAA| Arun    AAA   | Arun    AAA|\n",
      "|      2|2025-01-04|        Meena|   Mouse|  800.0|Meena    AAA   |Meena    AAA|Meena    AAA   |Meena    AAA|\n",
      "|      3|2025-01-05|         John|Keyboard| 1500.0| John    AAA   | John    AAA| John    AAA   | John    AAA|\n",
      "|      4|2025-01-06|        Priya| Monitor| 7200.0|Priya    AAA   |Priya    AAA|Priya    AAA   |Priya    AAA|\n",
      "+-------+----------+-------------+--------+-------+---------------+------------+---------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#concat\n",
    "df1 = df1.withColumn(\"full_name\",concat_ws(\" \",col(\"customer_name\"),lit(\"   AAA   \")))\n",
    "# df1.show(4)\n",
    "\n",
    "#trim\n",
    "df1 = df1.withColumn(\"trim_name\",trim(col(\"full_name\")))\n",
    "\n",
    "#ltrim\n",
    "df1 = df1.withColumn(\"ltrim_name\",ltrim(col(\"full_name\")))\n",
    "\n",
    "#rtrim\n",
    "df1 = df1.withColumn(\"rtrim_name\",rtrim(col(\"full_name\")))\n",
    "df1.show(4)\n",
    "df1 = df1.drop('full_name','trim_name','ltrim_name','rtrim_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea3399",
   "metadata": {},
   "source": [
    "#### Date fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0fdac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+------------+-------------+---------+----------+\n",
      "|      date|changed_date|current_date|after_7_days|before_7_days|date_diff|  last_day|\n",
      "+----------+------------+------------+------------+-------------+---------+----------+\n",
      "|2025-01-03|  03-01-2025|  2025-12-08|  2025-12-15|   2025-12-01|        7|2025-12-31|\n",
      "|2025-01-04|  04-01-2025|  2025-12-08|  2025-12-15|   2025-12-01|        7|2025-12-31|\n",
      "|2025-01-05|  05-01-2025|  2025-12-08|  2025-12-15|   2025-12-01|        7|2025-12-31|\n",
      "|2025-01-06|  06-01-2025|  2025-12-08|  2025-12-15|   2025-12-01|        7|2025-12-31|\n",
      "+----------+------------+------------+------------+-------------+---------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current_date\n",
    "df1 = df1.withColumn(\"current_date\",current_date())\n",
    "\n",
    "#date_add\n",
    "df1 = df1.withColumn(\"after_7_days\",date_add(\"current_date\",7))\n",
    "\n",
    "#date_sub\n",
    "df1 =df1.withColumn(\"before_7_days\",date_sub(\"current_date\",7))\n",
    "\n",
    "#date_diff\n",
    "df1 = df1.withColumn(\"date_diff\",date_diff(\"after_7_days\",\"current_date\"))\n",
    "\n",
    "#last_day\n",
    "df1 = df1.withColumn(\"last_day\",last_day(\"current_date\"))\n",
    "\n",
    "#date_format\n",
    "df1 = df1.withColumn(\"changed_date\",date_format(col('date'),'dd-MM-yyyy'))\n",
    "\n",
    "df1.select(\"date\",\"changed_date\",\"current_date\",\"after_7_days\",\"before_7_days\",\"date_diff\",\"last_day\",).show(4)\n",
    "df1 = df1.drop(\"changed_date\",\"current_date\",\"after_7_days\",\"before_7_days\",\"date_diff\",\"last_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98478d0b",
   "metadata": {},
   "source": [
    "#### handling null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66016d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sale_id: int, date: date, customer_name: string, product: string, amount: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dropna('any')\n",
    "df1.dropna('all')\n",
    "df1.dropna(subset=[\"customer_name\"])\n",
    "df1.fillna(\"not available\")\n",
    "df1.fillna(\"Not\",subset=[\"customer_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d722cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "data_f = spark.read.format('csv')\\\n",
    "                .option(\"inferSchema\",True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(\"C:/Git files/My git files/PySpark/files/friday_sale.csv\")\n",
    "data_f = data_f.withColumn(\"Full_name\",concat_ws(\" \",col(\"first_name\"),col(\"last_name\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511010f",
   "metadata": {},
   "source": [
    "#### split, indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a350230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+\n",
      "|     split_name|f_name|l_name|\n",
      "+---------------+------+------+\n",
      "|  [Arun, Kumar]|  Arun| Kumar|\n",
      "|[Meena, Sharma]| Meena|Sharma|\n",
      "| [John, Mathew]|  John|Mathew|\n",
      "| [Priya, Reddy]| Priya| Reddy|\n",
      "+---------------+------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_f = data_f.withColumn(\"split_name\",split(col(\"Full_name\"),\" \"))\n",
    "data_f = data_f.withColumn(\"f_name\",split(col(\"Full_name\"),\" \")[0])\\\n",
    "               .withColumn(\"l_name\",split(col(\"Full_name\"),\" \")[1])\n",
    "data_f.select(\"split_name\",\"f_name\",\"l_name\").show(4)\n",
    "data_f = data_f.drop(\"f_name\",\"l_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8cea2",
   "metadata": {},
   "source": [
    "#### Exploade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd03a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---------+--------+------+------------+---------------+------------+\n",
      "|sale_id|      date|first_name|last_name| product|amount|   Full_name|     split_name|explode_data|\n",
      "+-------+----------+----------+---------+--------+------+------------+---------------+------------+\n",
      "|      1|2025-01-03|      Arun|    Kumar|  Laptop| 56000|  Arun Kumar|  [Arun, Kumar]|        Arun|\n",
      "|      1|2025-01-03|      Arun|    Kumar|  Laptop| 56000|  Arun Kumar|  [Arun, Kumar]|       Kumar|\n",
      "|      2|2025-01-04|     Meena|   Sharma|   Mouse|   800|Meena Sharma|[Meena, Sharma]|       Meena|\n",
      "|      2|2025-01-04|     Meena|   Sharma|   Mouse|   800|Meena Sharma|[Meena, Sharma]|      Sharma|\n",
      "|      3|2025-01-05|      John|   Mathew|Keyboard|  1500| John Mathew| [John, Mathew]|        John|\n",
      "|      3|2025-01-05|      John|   Mathew|Keyboard|  1500| John Mathew| [John, Mathew]|      Mathew|\n",
      "+-------+----------+----------+---------+--------+------+------------+---------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_f = data_f.withColumn(\"explode_data\",explode(col(\"split_name\")))\n",
    "data_f.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f1177",
   "metadata": {},
   "source": [
    "#### Array Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4708232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---------+-------+------+------------+---------------+------------+--------------+\n",
      "|sale_id|      date|first_name|last_name|product|amount|   Full_name|     split_name|explode_data|array_contains|\n",
      "+-------+----------+----------+---------+-------+------+------------+---------------+------------+--------------+\n",
      "|      1|2025-01-03|      Arun|    Kumar| Laptop| 56000|  Arun Kumar|  [Arun, Kumar]|        Arun|          true|\n",
      "|      1|2025-01-03|      Arun|    Kumar| Laptop| 56000|  Arun Kumar|  [Arun, Kumar]|       Kumar|          true|\n",
      "|      2|2025-01-04|     Meena|   Sharma|  Mouse|   800|Meena Sharma|[Meena, Sharma]|       Meena|         false|\n",
      "|      2|2025-01-04|     Meena|   Sharma|  Mouse|   800|Meena Sharma|[Meena, Sharma]|      Sharma|         false|\n",
      "+-------+----------+----------+---------+-------+------+------------+---------------+------------+--------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- Full_name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_f = data_f.withColumn(\"array_contains\",array_contains(col(\"split_name\"),\"Kumar\"))\n",
    "data_f.show(4)\n",
    "data_f = data_f.drop(\"array_contains\",\"explode_data\",\"split_name\")\n",
    "data_f.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb902c4",
   "metadata": {},
   "source": [
    "#### Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b19abc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-------------+----------+\n",
      "|  product|maximun_amount|minmun_amount|sum_amount|\n",
      "+---------+--------------+-------------+----------+\n",
      "|Earphones|          1200|         1100|      4600|\n",
      "|  Speaker|          2600|         2500|     10200|\n",
      "|   Webcam|          3000|         2800|     11600|\n",
      "|   Laptop|         60000|        53000|    456000|\n",
      "|    Mouse|           850|          800|      3300|\n",
      "|   Tablet|         18000|        17500|     71000|\n",
      "|  Printer|          9500|         9000|     37000|\n",
      "| Keyboard|          1600|         1500|      6200|\n",
      "|  Monitor|          7200|         7100|     28600|\n",
      "+---------+--------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_f.groupBy(\"product\").agg(max(\"amount\").alias(\"maximun_amount\")\\\n",
    "                             ,min(\"amount\").alias(\"minmun_amount\")\\\n",
    "                             ,sum(\"amount\").alias(\"sum_amount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb9144",
   "metadata": {},
   "source": [
    "#### collect list,set,struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28990232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list,collect_set,struct\n",
    "\n",
    "spark = SparkSession.builder.appName(\"collect\").getOrCreate()\n",
    "\n",
    "student_hobbies = [\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",  \"hobby\": \"Cricket\"},\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",  \"hobby\": \"Music\"},\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",  \"hobby\": \"Chess\"},\n",
    "\n",
    "    {\"student_id\": 2, \"name\": \"Meena\", \"hobby\": \"Reading\"},\n",
    "    {\"student_id\": 2, \"name\": \"Meena\", \"hobby\": \"Dancing\"},\n",
    "\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\", \"hobby\": \"Swimming\"},\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\", \"hobby\": \"Cricket\"},\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\", \"hobby\": \"Gym\"},\n",
    "\n",
    "    {\"student_id\": 4, \"name\": \"Divya\", \"hobby\": \"Yoga\"},\n",
    "    {\"student_id\": 4, \"name\": \"Divya\", \"hobby\": \"Painting\"}\n",
    "]\n",
    "\n",
    "df_collect  = spark.createDataFrame(student_hobbies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db5baf",
   "metadata": {},
   "source": [
    "#### collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b753576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------------+\n",
      "|student_id|name |hobbies                 |\n",
      "+----------+-----+------------------------+\n",
      "|1         |Arun |[Cricket, Music, Chess] |\n",
      "|2         |Meena|[Reading, Dancing]      |\n",
      "|3         |Kiran|[Swimming, Cricket, Gym]|\n",
      "|4         |Divya|[Yoga, Painting]        |\n",
      "+----------+-----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_collect.groupBy(\"student_id\",\"name\")\\\n",
    "          .agg(collect_list(\"hobby\").alias(\"hobbies\"))\\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54584455",
   "metadata": {},
   "source": [
    "#### collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1eb5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------------+\n",
      "|student_id|name |hobbies                 |\n",
      "+----------+-----+------------------------+\n",
      "|1         |Arun |[Chess, Cricket, Music] |\n",
      "|2         |Meena|[Reading, Dancing]      |\n",
      "|3         |Kiran|[Swimming, Cricket, Gym]|\n",
      "|4         |Divya|[Painting, Yoga]        |\n",
      "+----------+-----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_collect.groupBy(\"student_id\",\"name\")\\\n",
    "          .agg(collect_set(\"hobby\").alias(\"hobbies\"))\\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bb0b9",
   "metadata": {},
   "source": [
    "#### struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79b0d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------+\n",
      "|name |details                                |\n",
      "+-----+---------------------------------------+\n",
      "|Arun |[{1, Cricket}, {1, Music}, {1, Chess}] |\n",
      "|Meena|[{2, Reading}, {2, Dancing}]           |\n",
      "|Kiran|[{3, Swimming}, {3, Cricket}, {3, Gym}]|\n",
      "|Divya|[{4, Yoga}, {4, Painting}]             |\n",
      "+-----+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_collect.select(\"name\",struct(\"student_id\",\"hobby\").alias(\"details\"))\\\n",
    "          .groupBy(\"name\")\\\n",
    "          .agg(collect_list(\"details\").alias(\"details\"))\\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746394",
   "metadata": {},
   "source": [
    "### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeb2f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import first,when,col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pivot\").getOrCreate()\n",
    "\n",
    "employee_attendance = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",  \"date\": \"2025-01-01\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",  \"date\": \"2025-01-02\", \"status\": \"Absent\"},\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",  \"date\": \"2025-01-03\", \"status\": \"Present\"},\n",
    "\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\", \"date\": \"2025-01-01\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\", \"date\": \"2025-01-02\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\", \"date\": \"2025-01-03\", \"status\": \"Present\"},\n",
    "\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\", \"date\": \"2025-01-01\", \"status\": \"Absent\"},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\", \"date\": \"2025-01-02\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\", \"date\": \"2025-01-03\", \"status\": \"Absent\"},\n",
    "\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\", \"date\": \"2025-01-01\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\", \"date\": \"2025-01-02\", \"status\": \"Absent\"},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\", \"date\": \"2025-01-03\", \"status\": \"Present\"}\n",
    "]\n",
    "\n",
    "df_pivot  = spark.createDataFrame(employee_attendance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5545759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+----------+----------+\n",
      "|emp_id| name|2025-01-01|2025-01-02|2025-01-03|\n",
      "+------+-----+----------+----------+----------+\n",
      "|   101| Arun|   Present|    Absent|   Present|\n",
      "|   102|Meena|   Present|   Present|   Present|\n",
      "|   103|Kiran|    Absent|   Present|    Absent|\n",
      "|   104|Divya|   Present|    Absent|   Present|\n",
      "+------+-----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_pivot.groupBy(\"emp_id\",\"name\")\\\n",
    "        .pivot(\"date\")\\\n",
    "        .agg(first(\"status\"))\n",
    "df.show(5)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c3c25",
   "metadata": {},
   "source": [
    "#### when otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4d24ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+----------+----------+------------+\n",
      "|emp_id| name|2025-01-01|2025-01-02|2025-01-03|student_type|\n",
      "+------+-----+----------+----------+----------+------------+\n",
      "|   101| Arun|   Present|    Absent|   Present| bad_student|\n",
      "|   102|Meena|   Present|   Present|   Present|good_student|\n",
      "|   103|Kiran|    Absent|   Present|    Absent| bad_student|\n",
      "|   104|Divya|   Present|    Absent|   Present| bad_student|\n",
      "+------+-----+----------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"student_type\",when(\n",
    "    (col(\"2025-01-01\")==\"Present\")&\n",
    "    (col(\"2025-01-02\")==\"Present\")&\n",
    "    (col(\"2025-01-03\")==\"Present\"),\"good_student\").otherwise(\"bad_student\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d0b9a",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ed4cd",
   "metadata": {},
   "source": [
    "#### inner,left,right,anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f28e236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner_join\n",
      "+----------+----+---+----------+-------+-----+\n",
      "|student_id|Name|Age|student_id|    sub|Marks|\n",
      "+----------+----+---+----------+-------+-----+\n",
      "|         1|Arun| 20|         1|  Maths|   88|\n",
      "|         1|Arun| 20|         1|Science|   92|\n",
      "|         3|Ravi| 19|         3|  Maths|   76|\n",
      "+----------+----+---+----------+-------+-----+\n",
      "\n",
      "left_join\n",
      "+----------+-----+---+----------+-------+-----+\n",
      "|student_id| Name|Age|student_id|    sub|Marks|\n",
      "+----------+-----+---+----------+-------+-----+\n",
      "|         1| Arun| 20|         1|Science|   92|\n",
      "|         1| Arun| 20|         1|  Maths|   88|\n",
      "|         2|Meena| 22|      NULL|   NULL| NULL|\n",
      "|         3| Ravi| 19|         3|  Maths|   76|\n",
      "|         4|Kiran| 21|      NULL|   NULL| NULL|\n",
      "+----------+-----+---+----------+-------+-----+\n",
      "\n",
      "right_join\n",
      "+----------+----+----+----------+-------+-----+\n",
      "|student_id|Name| Age|student_id|    sub|Marks|\n",
      "+----------+----+----+----------+-------+-----+\n",
      "|         1|Arun|  20|         1|  Maths|   88|\n",
      "|         1|Arun|  20|         1|Science|   92|\n",
      "|         3|Ravi|  19|         3|  Maths|   76|\n",
      "|      NULL|NULL|NULL|         5|Science|   81|\n",
      "+----------+----+----+----------+-------+-----+\n",
      "\n",
      "anti_join\n",
      "+----------+-----+---+\n",
      "|student_id| Name|Age|\n",
      "+----------+-----+---+\n",
      "|         2|Meena| 22|\n",
      "|         4|Kiran| 21|\n",
      "+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"join\").getOrCreate()\n",
    "data1 = [\n",
    "  (1, \"Arun\",  20),\n",
    "  (2, \"Meena\", 22),\n",
    "  (3, \"Ravi\",  19),\n",
    "  (4, \"Kiran\", 21)\n",
    "]\n",
    "\n",
    "data2 =[\n",
    "  (1, \"Maths\",     88),\n",
    "  (1, \"Science\",   92),\n",
    "  (3, \"Maths\",     76),\n",
    "  (5, \"Science\",   81) \n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1,schema=['student_id',\"Name\",\"Age\"])\n",
    "df2 = spark.createDataFrame(data=data2,schema=['student_id',\"sub\",\"Marks\"])\n",
    "\n",
    "print(\"Inner_join\")\n",
    "df_inner = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='inner')\n",
    "df_inner.show()\n",
    "\n",
    "print(\"left_join\")\n",
    "df_left = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='left')\n",
    "df_left.show()\n",
    "\n",
    "print(\"right_join\")\n",
    "df_right = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='right')\n",
    "df_right.show()\n",
    "\n",
    "print(\"anti_join\")\n",
    "df_right = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='anti')\n",
    "df_right.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf4e3d",
   "metadata": {},
   "source": [
    "#### Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a508010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+------+--------+\n",
      "|emp_id| name|   dept|rating|projects|\n",
      "+------+-----+-------+------+--------+\n",
      "|   110|Rahul|Finance|   4.0|       4|\n",
      "|   109|Anita|     HR|   3.7|       2|\n",
      "|   101| Arun|     IT|   4.3|       5|\n",
      "|   105|Priya|  Sales|   4.1|       4|\n",
      "+------+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, rank,dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "employee_perf = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",   \"dept\": \"IT\",        \"rating\": 4.3, \"projects\": 5},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",  \"dept\": \"HR\",        \"rating\": 3.8, \"projects\": 3},\n",
    "    {\"emp_id\": 103, \"name\": \"Sachin\", \"dept\": \"Finance\",   \"rating\": 4.7, \"projects\": 6},\n",
    "    {\"emp_id\": 104, \"name\": \"Kiran\",  \"dept\": \"IT\",        \"rating\": 3.9, \"projects\": 2},\n",
    "    {\"emp_id\": 105, \"name\": \"Priya\",  \"dept\": \"Sales\",     \"rating\": 4.1, \"projects\": 4},\n",
    "    {\"emp_id\": 106, \"name\": \"Akash\",  \"dept\": \"Sales\",     \"rating\": 4.8, \"projects\": 7},\n",
    "    {\"emp_id\": 107, \"name\": \"Deepa\",  \"dept\": \"Finance\",   \"rating\": 3.6, \"projects\": 3},\n",
    "    {\"emp_id\": 108, \"name\": \"Varun\",  \"dept\": \"IT\",        \"rating\": 4.5, \"projects\": 5},\n",
    "    {\"emp_id\": 109, \"name\": \"Anita\",  \"dept\": \"HR\",        \"rating\": 3.7, \"projects\": 2},\n",
    "    {\"emp_id\": 110, \"name\": \"Rahul\",  \"dept\": \"Finance\",   \"rating\": 4.0, \"projects\": 4},\n",
    "    {\"emp_id\": 111, \"name\": \"Teena\", \"dept\": \"IT\", \"rating\": 4.3, \"projects\": 4}\n",
    "\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=employee_perf)\n",
    "\n",
    "win = Window.partitionBy(\"dept\").orderBy(col(\"rating\").desc())\n",
    "\n",
    "df = df.withColumn(\"row_number\",row_number().over(win))\\\n",
    "       .withColumn(\"rank\",rank().over(win))\\\n",
    "       .withColumn(\"dense_rank\",dense_rank().over(win))\n",
    "\n",
    "df.filter((col(\"row_number\")==2) & (col(\"rank\")==2) & (col(\"dense_rank\")==2)).select(\"emp_id\",\"name\",\"dept\",\"rating\",\"projects\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6be7a1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|salesperson|total_sales|\n",
      "+-----------+-----------+\n",
      "|       Arun|      13000|\n",
      "|      Meena|      16000|\n",
      "|     Sachin|      14000|\n",
      "|      Priya|      10000|\n",
      "|      Kiran|       3000|\n",
      "|      Deepa|       5500|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+-----------+----+\n",
      "|salesperson|total_sales|rank|\n",
      "+-----------+-----------+----+\n",
      "|      Meena|      16000|   1|\n",
      "|     Sachin|      14000|   2|\n",
      "+-----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 5000},\n",
    "    {\"sale_id\": 2, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 7000},\n",
    "    {\"sale_id\": 3, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 6500},\n",
    "    {\"sale_id\": 4, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 8000},\n",
    "    {\"sale_id\": 5, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 4000},\n",
    "    {\"sale_id\": 6, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 9000},\n",
    "    {\"sale_id\": 7, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 7500},\n",
    "    {\"sale_id\": 8, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 6000},\n",
    "    {\"sale_id\": 9, \"region\": \"North\", \"salesperson\": \"Kiran\",  \"amount\": 3000},\n",
    "    {\"sale_id\": 10,\"region\": \"South\", \"salesperson\": \"Deepa\",  \"amount\": 5500}\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data=sales_data)\n",
    "\n",
    "win = Window.partitionBy(\"region\").orderBy(F.col(\"amount\").desc())\n",
    "win2 = Window.partitionBy(\"salesperson\").orderBy(F.col(\"amount\").desc())\n",
    "\n",
    "df1 = df1.withColumn(\"row_number\",F.row_number().over(win))\n",
    "\n",
    "# df1.filter(col(\"row_number\")==1).show()\n",
    "\n",
    "df1 = df1.groupBy(\"salesperson\") \\\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales\"))\n",
    "\n",
    "df1.show()\n",
    "win2 = Window.orderBy(F.col(\"total_sales\").desc())\n",
    "\n",
    "df1 = df1.withColumn(\"rank\", F.rank().over(win2))\n",
    "\n",
    "df1.filter((F.col(\"rank\")==1) | (F.col(\"rank\")==2)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f9be7",
   "metadata": {},
   "source": [
    "### UDF ->User defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f711852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+--------+-----------+\n",
      "|   category|price|prod_id| product|price_range|\n",
      "+-----------+-----+-------+--------+-----------+\n",
      "|Electronics|56000|      1|  Laptop|       High|\n",
      "|Electronics| 1500|      2|Keyboard|        Low|\n",
      "|    Fashion| 2500|      3|   Shoes|     Medium|\n",
      "|    Fashion|  700|      4| T-Shirt|        Low|\n",
      "|       Home| 3200|      5|   Mixer|     Medium|\n",
      "|       Home|28000|      6|    Sofa|       High|\n",
      "+-----------+-----+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "\n",
    "products = [\n",
    "    {\"prod_id\": 1, \"product\": \"Laptop\",\n",
    "        \"category\": \"Electronics\", \"price\": 56000},\n",
    "    {\"prod_id\": 2, \"product\": \"Keyboard\",\n",
    "        \"category\": \"Electronics\", \"price\": 1500},\n",
    "    {\"prod_id\": 3, \"product\": \"Shoes\",      \"category\": \"Fashion\",     \"price\": 2500},\n",
    "    {\"prod_id\": 4, \"product\": \"T-Shirt\",    \"category\": \"Fashion\",     \"price\": 700},\n",
    "    {\"prod_id\": 5, \"product\": \"Mixer\",      \"category\": \"Home\",        \"price\": 3200},\n",
    "    {\"prod_id\": 6, \"product\": \"Sofa\",       \"category\": \"Home\",        \"price\": 28000}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(products)\n",
    "\n",
    "\n",
    "def price_range(x):\n",
    "    if x < 2000:\n",
    "        return \"Low\"\n",
    "    elif x >= 2000 and x <= 10000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "\n",
    "my_price_range = F.udf(price_range)\n",
    "\n",
    "df.withColumn(\"price_range\", my_price_range(F.col(\"price\"))).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
