{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2a4f0d",
   "metadata": {},
   "source": [
    "## UDF ->USER Defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 5000},\n",
    "    {\"sale_id\": 2, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 7000},\n",
    "    {\"sale_id\": 3, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 6500},\n",
    "    {\"sale_id\": 4, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 8000},\n",
    "    {\"sale_id\": 5, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 4000},\n",
    "    {\"sale_id\": 6, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 9000},\n",
    "    {\"sale_id\": 7, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 7500},\n",
    "    {\"sale_id\": 8, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 6000},\n",
    "    {\"sale_id\": 9, \"region\": \"North\", \"salesperson\": \"Kiran\",  \"amount\": 3000},\n",
    "    {\"sale_id\": 10,\"region\": \"South\", \"salesperson\": \"Deepa\",  \"amount\": 5500}\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data=sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70a0a0",
   "metadata": {},
   "source": [
    "### step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x,y):\n",
    "    if x>5000 and y in (\"North\",\"East\"):\n",
    "        return x//100\n",
    "    else:\n",
    "        return x//50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e49df8",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45282f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_udf = F.udf(my_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ae1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.withColumn(\"Bonus\",my_udf(F.col(\"amount\"),F.col(\"region\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92291ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 5000},\n",
    "    {\"sale_id\": 2, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 7000},\n",
    "    {\"sale_id\": 3, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 6500},\n",
    "    {\"sale_id\": 4, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 8000},\n",
    "    {\"sale_id\": 5, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 4000},\n",
    "    {\"sale_id\": 6, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 9000},\n",
    "    {\"sale_id\": 7, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 7500},\n",
    "    {\"sale_id\": 8, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 6000},\n",
    "    {\"sale_id\": 9, \"region\": \"North\", \"salesperson\": \"Kiran\",  \"amount\": 3000},\n",
    "    {\"sale_id\": 10,\"region\": \"South\", \"salesperson\": \"Deepa\",  \"amount\": 5500}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=sales_data)\n",
    "\n",
    "def add_bouns_based_region(x,y):\n",
    "    if x in (\"North\",\"South\"):\n",
    "        return y*0.1\n",
    "    else:\n",
    "        return y*0.05\n",
    "\n",
    "my_fun = F.udf(add_bouns_based_region)\n",
    "\n",
    "df.withColumn(\"bonus\",my_fun(F.col(\"region\"),F.col(\"amount\")))\\\n",
    "  .withColumn(\"Bonus %\",F.when(((F.col(\"region\")==\"North\") | (F.col(\"region\")==\"South\")),\"10% bonus\")\\\n",
    "    .otherwise(\"5% bonus\")).show()\n",
    "\n",
    "#or \n",
    "\n",
    "df.withColumn(\"bonus\",my_fun(F.col(\"region\"),F.col(\"amount\")))\\\n",
    "  .withColumn(\"Bonus %\",F.when(F.col(\"region\").isin(\"North\",\"South\"),\"10% bonus\").otherwise(\"5% bonus\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "employees = [\n",
    "    {\"emp_id\": 1, \"name\": \"Arun\",  \"dept\": \"IT\",       \"experience\": 2,  \"salary\": 45000},\n",
    "    {\"emp_id\": 2, \"name\": \"Meena\", \"dept\": \"HR\",       \"experience\": 6,  \"salary\": 65000},\n",
    "    {\"emp_id\": 3, \"name\": \"Kiran\", \"dept\": \"IT\",       \"experience\": 4,  \"salary\": 55000},\n",
    "    {\"emp_id\": 4, \"name\": \"Sachin\",\"dept\": \"Finance\",  \"experience\": 8,  \"salary\": 82000},\n",
    "    {\"emp_id\": 5, \"name\": \"Priya\", \"dept\": \"Finance\",  \"experience\": 1,  \"salary\": 40000},\n",
    "    {\"emp_id\": 6, \"name\": \"Deepa\", \"dept\": \"HR\",       \"experience\": 10, \"salary\": 90000},\n",
    "    {\"emp_id\": 7, \"name\": \"John\",  \"dept\": \"Marketing\",\"experience\": 3,  \"salary\": 48000},\n",
    "    {\"emp_id\": 8, \"name\": \"Riya\",  \"dept\": \"Marketing\",\"experience\": 7,  \"salary\": 70000}\n",
    "]\n",
    "\n",
    "spark  = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(employees)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without udf\n",
    "df.withColumn(\n",
    "    \"experience_level\",\n",
    "    F.when((F.col(\"experience\") > 0) & (F.col(\"experience\") <= 3), \"Junior\")\n",
    "     .when((F.col(\"experience\") > 3) & (F.col(\"experience\") <= 7), \"Mid\")\n",
    "     .otherwise(\"Senior\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf\n",
    "def exp(x):\n",
    "    if x>0 and x<=3:\n",
    "        return \"Junior\"\n",
    "    elif x>3 and x<=7:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "my_fun = F.udf(exp)\n",
    "\n",
    "#with udf\n",
    "df.withColumn(\"experience_level\",my_fun(F.col(\"experience\"))).show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
