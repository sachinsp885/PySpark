{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from  pyspark.sql.window import Window\n",
    "\n",
    "employees = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",    \"dept\": \"IT\",      \"salary\": 80000, \"experience\": 5,  \"skills\": [\"Python\", \"SQL\"],      \"status\": \"Active\",   \"hire_date\": \"2021-01-10\"},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",   \"dept\": \"HR\",      \"salary\": 55000, \"experience\": 3,  \"skills\": [\"Excel\"],               \"status\": \"Active\",   \"hire_date\": \"2022-03-15\"},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\",   \"dept\": \"IT\",      \"salary\": 90000, \"experience\": 7,  \"skills\": [\"Python\", \"PySpark\"],  \"status\": \"Resigned\", \"hire_date\": \"2018-10-01\"},\n",
    "    {\"emp_id\": 104, \"name\": \"Bhavani\", \"dept\": \"Finance\", \"salary\": 65000, \"experience\": 4,  \"skills\": [\"Excel\", \"SQL\"],        \"status\": \"Active\",   \"hire_date\": \"2020-07-19\"},\n",
    "    {\"emp_id\": 105, \"name\": \"Akash\",   \"dept\": \"IT\",      \"salary\": 75000, \"experience\": 2,  \"skills\": [\"Python\"],              \"status\": \"Active\",   \"hire_date\": \"2023-05-02\"},\n",
    "    \n",
    "    {\"emp_id\": 106, \"name\": \"Ravi\",    \"dept\": \"Sales\",   \"salary\": 45000, \"experience\": 1,  \"skills\": [],                      \"status\": \"Active\",   \"hire_date\": \"2024-01-11\"},\n",
    "    {\"emp_id\": 107, \"name\": \"Divya\",   \"dept\": \"HR\",      \"salary\": None,  \"experience\": 6,  \"skills\": [\"Excel\", \"PowerBI\"],    \"status\": \"Active\",   \"hire_date\": \"2019-11-28\"},\n",
    "    {\"emp_id\": 108, \"name\": \"Suresh\",  \"dept\": \"Sales\",   \"salary\": 47000, \"experience\": 2,  \"skills\": [\"Negotiation\"],         \"status\": \"Active\",   \"hire_date\": \"2021-06-22\"},\n",
    "    {\"emp_id\": 109, \"name\": \"Jaya\",    \"dept\": \"IT\",      \"salary\": 80000, \"experience\": 5,  \"skills\": [\"SQL\"],                 \"status\": \"Active\",   \"hire_date\": \"2020-09-10\"},\n",
    "    {\"emp_id\": 110, \"name\": \"Vijay\",   \"dept\": \"Finance\", \"salary\": 70000, \"experience\": 8,  \"skills\": [\"Excel\"],               \"status\": \"Resigned\", \"hire_date\": \"2017-12-01\"},\n",
    "\n",
    "    {\"emp_id\": 111, \"name\": \"Rohit\",   \"dept\": \"IT\",      \"salary\": 65000, \"experience\": 3,  \"skills\": [\"Python\"],              \"status\": \"Active\",   \"hire_date\": \"2022-02-17\"},\n",
    "    {\"emp_id\": 112, \"name\": \"Maya\",    \"dept\": \"HR\",      \"salary\": 62000, \"experience\": 4,  \"skills\": None,                    \"status\": \"Active\",   \"hire_date\": None},\n",
    "    {\"emp_id\": 113, \"name\": \"Goutham\", \"dept\": \"Sales\",   \"salary\": 52000, \"experience\": 3,  \"skills\": [\"Negotiation\", \"CRM\"],  \"status\": \"Active\",   \"hire_date\": \"2021-08-05\"},\n",
    "    {\"emp_id\": 114, \"name\": \"Lavanya\", \"dept\": \"IT\",      \"salary\": 90000, \"experience\": 10, \"skills\": [\"Python\", \"AWS\"],      \"status\": \"Active\",   \"hire_date\": \"2015-07-14\"},\n",
    "    {\"emp_id\": 115, \"name\": \"Kavya\",   \"dept\": \"Finance\", \"salary\": 58000, \"experience\": 1,  \"skills\": [\"Excel\"],               \"status\": \"Active\",   \"hire_date\": \"2023-01-20\"},\n",
    "\n",
    "    {\"emp_id\": 116, \"name\": \"Manish\",  \"dept\": \"IT\",      \"salary\": None,  \"experience\": 4,  \"skills\": [\"SQL\"],                 \"status\": \"Active\",   \"hire_date\": \"2020-10-05\"},\n",
    "    {\"emp_id\": 117, \"name\": \"Rakesh\",  \"dept\": \"Sales\",   \"salary\": 48000, \"experience\": 2,  \"skills\": [\"CRM\"],                 \"status\": None,       \"hire_date\": \"2022-11-11\"},\n",
    "    {\"emp_id\": 118, \"name\": \"Anita\",   \"dept\": \"HR\",      \"salary\": 56000, \"experience\": 3,  \"skills\": [\"Excel\"],               \"status\": \"Active\",   \"hire_date\": \"2023-03-01\"},\n",
    "    {\"emp_id\": 119, \"name\": \"Tarun\",   \"dept\": \"Finance\", \"salary\": 70000, \"experience\": 7,  \"skills\": [\"SQL\", \"PowerBI\"],      \"status\": \"Active\",   \"hire_date\": \"2018-02-25\"},\n",
    "    {\"emp_id\": 120, \"name\": \"Sanjay\",  \"dept\": \"IT\",      \"salary\": 76000, \"experience\": 5,  \"skills\": [\"Python\", \"SQL\"],       \"status\": \"Active\",   \"hire_date\": \"2019-09-09\"}\n",
    "]\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Question 1\n",
    "# Write PySpark code to filter only employees from the IT department whose salary is greater than 75,000.\n",
    "\n",
    "df.filter((F.col(\"dept\")=='IT') & (F.col(\"salary\") >75000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a009eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:\n",
    "# Write a PySpark query to find all employees who joined in or after the year 2021.\n",
    "\n",
    "df.filter(F.year(F.col(\"hire_date\"))>=2021).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find employees who have Python in their skills.\n",
    "df.filter(F.array_contains(\"skills\",\"Python\")).select('name','skills').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest-paid employee in each department using a window function.\n",
    "# Return: emp_id, name, dept, salary, rank.\n",
    "\n",
    "win = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df1 = df.withColumn(\"rank\",F.rank().over(win))\\\n",
    "  .withColumn(\"dense_rank\",F.dense_rank().over(win))\n",
    "df.filter(F.col(\"rank\")==1).select(\"emp_id\",\"name\",\"dept\",\"salary\",\"rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark query to explode the skills array so that each skill appears in a separate row.\n",
    "# Return only:\n",
    "# emp_id, name, skill\n",
    "\n",
    "df2 = df.withColumn(\"skill\",F.explode(\"skills\"))\n",
    "df2.select('emp_id','name','skill').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1124a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark query to replace all null salaries with 0.\n",
    "df.fillna(0,subset=[\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark query to count how many employees are present in each department.\n",
    "# Return columns: dept, emp_count\n",
    "\n",
    "df3 = df.groupBy(\"dept\").agg(F.count(\"name\").alias(\"number_of_emp\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some employees have status = null.\n",
    "# Write a PySpark query to replace null status with \"Unknown\" using when and otherwise.\n",
    "\n",
    "df4 =df.withColumn(\"status\",F.when(F.col(\"status\").isNull(),\"Unknown\").otherwise(F.col(\"status\")))\n",
    "df4.filter(F.col(\"status\")=='Unknown').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ed511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average salary of each department, ignoring null salaries.\n",
    "# Return: dept, avg_salary\n",
    "\n",
    "df5 = df.filter(F.col(\"salary\").isNotNull())\n",
    "df5 =df5.groupBy(\"dept\").agg(F.mean(\"salary\").alias(\"avg_salary\"))\n",
    "df5.select('dept','avg_salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50468360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find the second highest salary in each department.\n",
    "# Return: dept, emp_id, name, salary.\n",
    "\n",
    "wind = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df6 = df.withColumn(\"rank\",F.rank().over(wind))\n",
    "df6.filter(F.col(\"rank\")==2).select('dept','emp_id','name','salary').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.withColumn(\"skill\",F.explode(\"skills\"))\n",
    "unique_skills_count = df7.select(\"skill\").distinct().count()  # count distinct skills\n",
    "print(unique_skills_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group employees by department and collect all employee names into a list for each department.\n",
    "# Return columns: dept, emp_list.\n",
    "df.groupBy(\"dept\").agg(F.collect_list('name')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea63931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_fixed = df.withColumn(\n",
    "    \"skills\",\n",
    "    F.when(F.col(\"skills\").isNull(), F.array()).otherwise(F.col(\"skills\"))\n",
    ")\n",
    "\n",
    "df_exploded = df_fixed.withColumn(\"skill\", F.explode(\"skills\"))\n",
    "\n",
    "df_exploded.select(\"emp_id\", \"name\", \"skill\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76724b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('hire_date',F.to_date(F.col(\"hire_date\"),'yyyy-MM-dd'))\n",
    "\n",
    "df = df.withColumn(\"days_with_company\",F.date_diff(F.current_date(),'hire_date'))\n",
    "\n",
    "df.select('emp_id', 'name', 'hire_date', 'days_with_company').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find employees whose name starts with 'A' or ends with 'y'.\n",
    "# Return: emp_id, name, dept.\n",
    "\n",
    "df.filter((F.col('name').startswith('A')) | (F.col('name').endswith('y')))\\\n",
    "  .select('emp_id','name','dept').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find all duplicate employee names and the count of how many times each name appears.\n",
    "# Return columns: name, count.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"name\") \\\n",
    "  .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "  .filter(F.col(\"count\") > 1) \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort employees first by department ascending, then salary descending.\n",
    "# Return columns: emp_id, name, dept, salary.\n",
    "\n",
    "df1 =df.sort((['dept','salary']),ascending=[1,0])\n",
    "df1.select('emp_id','name','dept','salary').show(5)\n",
    "\n",
    "df1 = df.orderBy(F.col(\"dept\").asc(), F.col(\"salary\").desc())\n",
    "df1.select('emp_id','name','dept','salary').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 2 highest-paid employees in each department.\n",
    "# Return columns: dept, emp_id, name, salary.\n",
    "\n",
    "\n",
    "w = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df.withColumn('rank',F.rank().over(w))\\\n",
    "  .filter(F.col(\"rank\")<=2).select(\"dept\", \"emp_id\", \"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null salaries with the average salary of their department.\n",
    "# Return columns: emp_id, name, dept, salary.\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window partitioned by department\n",
    "win = Window.partitionBy(\"dept\")\n",
    "\n",
    "# Replace null salaries with average salary of the department\n",
    "df_filled = df.withColumn(\n",
    "    \"salary\",\n",
    "    F.when(\n",
    "        F.col(\"salary\").isNull(),\n",
    "        F.round(F.avg(\"salary\").over(win), 2)  # round to 2 decimals (optional)\n",
    "    ).otherwise(F.col(\"salary\"))\n",
    ")\n",
    "\n",
    "# df_filled.select(\"emp_id\", \"name\", \"dept\", \"salary\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window to get dept-wise average salary\n",
    "win = Window.partitionBy(\"dept\")\n",
    "\n",
    "df_filled = df.withColumn(\n",
    "    \"salary\",\n",
    "    F.coalesce(F.col(\"salary\"), F.round(F.avg(\"salary\").over(win),2))\n",
    ")\n",
    "\n",
    "df_filled.select(\"emp_id\", \"name\", \"dept\", \"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('skill',F.explode('skills'))\n",
    "df.groupBy('skill').agg(F.count('name').alias('emp_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sales_data = [\n",
    "    {\"txn_id\": 1, \"cust_id\": 101, \"amount\": 500,  \"status\": \"success\", \"txn_date\": \"2024-01-01\"},\n",
    "    {\"txn_id\": 2, \"cust_id\": 102, \"amount\": None, \"status\": \"success\", \"txn_date\": \"2024-01-01\"},\n",
    "    {\"txn_id\": 3, \"cust_id\": 101, \"amount\": 200,  \"status\": \"failed\",  \"txn_date\": \"2024-01-03\"},\n",
    "    {\"txn_id\": 4, \"cust_id\": 103, \"amount\": 800,  \"status\": \"success\", \"txn_date\": \"2024-01-04\"},\n",
    "    {\"txn_id\": 5, \"cust_id\": 104, \"amount\": None, \"status\": \"failed\",  \"txn_date\": \"2024-01-05\"}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# Replace null amount with 0\n",
    "df = df.fillna(0,subset=['amount'])\n",
    "\n",
    "# Convert txn_date into proper date type\n",
    "df = df.withColumn('txn_date',to_date(col('txn_date'),'yyyy-MM-dd'))\n",
    "# df = df.withColumn('hire_date',F.to_date(F.col(\"hire_date\"),'yyyy-MM-dd'))\n",
    "\n",
    "# Add a new column is_success →\n",
    "# 1 if status = \"success\"\n",
    "#  ->0 otherwise\n",
    "df = df.withColumn(\"is_success\",when(col('status')=='success',1).otherwise(0))\n",
    "\n",
    "# Filter only transactions where amount > 0\n",
    "df = df.filter(col('amount')>0)\n",
    "\n",
    "# Final output columns:\n",
    "# txn_id, cust_id, amount, txn_date, is_success\n",
    "df.select('txn_id', 'cust_id', 'amount', 'txn_date', 'is_success').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "92291926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+-----------+----------+\n",
      "|order_id|cust_id|amount|   category|order_date|\n",
      "+--------+-------+------+-----------+----------+\n",
      "|       1|    101| 500.0|Electronics|2024-01-01|\n",
      "|       3|    102| 900.0|Electronics|2024-01-05|\n",
      "|       5|    103| 600.0|    Fashion|2024-01-07|\n",
      "|       6|    101| 200.0|    Fashion|2024-01-09|\n",
      "|       2|    101| 300.0|    Grocery|2024-01-03|\n",
      "|       4|    103| 300.0|    Grocery|2024-01-06|\n",
      "+--------+-------+------+-----------+----------+\n",
      "\n",
      "+-------+------------+\n",
      "|cust_id|total amount|\n",
      "+-------+------------+\n",
      "|    103|       900.0|\n",
      "|    101|      1000.0|\n",
      "|    102|       900.0|\n",
      "+-------+------------+\n",
      "\n",
      "+-----------+------+\n",
      "|   category|amount|\n",
      "+-----------+------+\n",
      "|Electronics| 900.0|\n",
      "|    Fashion| 600.0|\n",
      "|    Grocery| 300.0|\n",
      "|    Grocery| 300.0|\n",
      "+-----------+------+\n",
      "\n",
      "+------+-----------+-------+----------+--------+----------------+\n",
      "|amount|   category|cust_id|order_date|order_id|days_since_order|\n",
      "+------+-----------+-------+----------+--------+----------------+\n",
      "| 500.0|Electronics|    101|2024-01-01|       1|      2025-12-06|\n",
      "| 900.0|Electronics|    102|2024-01-05|       3|      2025-12-06|\n",
      "| 600.0|    Fashion|    103|2024-01-07|       5|      2025-12-06|\n",
      "| 200.0|    Fashion|    101|2024-01-09|       6|      2025-12-06|\n",
      "| 300.0|    Grocery|    101|2024-01-03|       2|      2025-12-06|\n",
      "| 300.0|    Grocery|    103|2024-01-06|       4|      2025-12-06|\n",
      "+------+-----------+-------+----------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "order_data = [\n",
    "    {\"order_id\": 1, \"cust_id\": 101, \"amount\": 500, \"category\": \"Electronics\", \"order_date\": \"2024-01-01\"},\n",
    "    {\"order_id\": 2, \"cust_id\": 101, \"amount\": 300, \"category\": \"Grocery\",     \"order_date\": \"2024-01-03\"},\n",
    "    {\"order_id\": 3, \"cust_id\": 102, \"amount\": 900, \"category\": \"Electronics\", \"order_date\": \"2024-01-05\"},\n",
    "    {\"order_id\": 4, \"cust_id\": 103, \"amount\": None,\"category\": \"Grocery\",     \"order_date\": \"2024-01-06\"},\n",
    "    {\"order_id\": 5, \"cust_id\": 103, \"amount\": 600, \"category\": \"Fashion\",     \"order_date\": \"2024-01-07\"},\n",
    "    {\"order_id\": 6, \"cust_id\": 101, \"amount\": 200, \"category\": \"Fashion\",     \"order_date\": \"2024-01-09\"}\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"interview\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(order_data)\n",
    "\n",
    "# 1️⃣ Replace null amount with the average amount of that category\n",
    "\n",
    "win = Window.partitionBy('category')\n",
    "df = df.withColumn(\"amount\",coalesce('amount',avg('amount').over(win)))\n",
    "df.select('order_id', 'cust_id', 'amount', 'category', 'order_date').show()\n",
    "\n",
    "\n",
    "# 2️⃣ Find total amount per customer\n",
    "df.groupBy('cust_id').agg(sum('amount').alias(\"total amount\")).show()\n",
    "\n",
    "# 3️⃣ For each category, find the highest amount order\n",
    "win = Window.partitionBy('category').orderBy(col('amount').desc())\n",
    "\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(win))\\\n",
    "  .filter(col(\"dense_rank\")==1)\\\n",
    "  .select('category',\"amount\").show()\n",
    "\n",
    "# 4️⃣ Add a new column days_since_order using current_date\n",
    "df.withColumn(\"days_since_order\",current_date()).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
