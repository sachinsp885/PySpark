{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6379c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CountryLogic\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Country\"])\n",
    "\n",
    "# 1️⃣ Get all unique countries\n",
    "all_countries = (\n",
    "    df.select(F.explode(F.split(\"Country\", \",\")))\n",
    "      .distinct()\n",
    "      .agg(F.collect_set(\"col\").alias(\"AllCountries\"))\n",
    ")\n",
    "\n",
    "all_countries.show(truncate=False)\n",
    "\n",
    "# 2️⃣ Split visited countries\n",
    "df2 = df.withColumn(\"VisitedCountryArr\", F.split(\"Country\", \",\"))\n",
    "df2.show()\n",
    "\n",
    "# 3️⃣ Cross join to get all countries for comparison\n",
    "df3 = df2.crossJoin(all_countries)\n",
    "df3.show(truncate=False)\n",
    "\n",
    "# 4️⃣ Find non visited countries\n",
    "result = df3.withColumn(\n",
    "    \"NonVisitedCountry\",\n",
    "    F.array_join(\n",
    "        F.array_except(F.col(\"AllCountries\"), F.col(\"VisitedCountryArr\")),\n",
    "        \",\"\n",
    "    )\n",
    ").select(\n",
    "    \"Name\",\n",
    "    F.col(\"Country\").alias(\"VisitedCountry\"),\n",
    "    \"NonVisitedCountry\"\n",
    ")\n",
    "\n",
    "result.show(truncate=False)\n",
    "\n",
    "# result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NoCrossJoin\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Country\"])\n",
    "\n",
    "# 1️⃣ Get all unique countries as a Python list\n",
    "all_countries = (\n",
    "    df.select(F.explode(F.split(\"Country\", \",\")))\n",
    "      .distinct()\n",
    "      .rdd.flatMap(lambda x: x)\n",
    "      .collect()\n",
    ")\n",
    "\n",
    "# 2️⃣ Create visited & non-visited columns\n",
    "result = (\n",
    "    df.withColumn(\"VisitedArr\", F.split(\"Country\", \",\"))\n",
    "      .withColumn(\n",
    "          \"NonVisitedCountry\",\n",
    "          F.array_join(\n",
    "              F.array_except(\n",
    "                  F.array(*[F.lit(c) for c in all_countries]),\n",
    "                  F.col(\"VisitedArr\")\n",
    "              ),\n",
    "              \",\"\n",
    "          )\n",
    "      )\n",
    "      .select(\n",
    "          \"Name\",\n",
    "          F.col(\"Country\").alias(\"VisitedCountry\"),\n",
    "          \"NonVisitedCountry\"\n",
    "      )\n",
    ")\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11943ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, explode, collect_set,\n",
    "    array_except, concat_ws\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    \n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"CountryName\"])\n",
    "df1 = df.withColumn(\"VisitedCountryArr\", split(col(\"CountryName\"), \",\"))\n",
    "df_exploded = df1.withColumn(\"country\", explode(col(\"VisitedCountryArr\")))\n",
    "w = Window.partitionBy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3167789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_all = df_exploded.withColumn(\n",
    "    \"AllCountries\",\n",
    "    collect_set(\"country\").over(w)\n",
    ")\n",
    "df_with_all.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df_with_all.dropDuplicates([\"Name\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\n",
    "    \"NonVisitedCountryArr\",\n",
    "    array_except(col(\"AllCountries\"), col(\"VisitedCountryArr\"))\n",
    ")\n",
    "\n",
    "df2.select('Name','VisitedCountryArr','NonVisitedCountryArr').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee617f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    " \n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "data =[(\"Ram\",30,\"Russia\"),\n",
    "\t   (\"Radha\",31,\"Russia,Norway\"),\n",
    "\t   (\"Kannu\",35,\"Norway,Belgium\")\n",
    "\t   ]\n",
    "df = spark.createDataFrame(data,[\"Name\",\"age\",\"Country\"])\n",
    "\n",
    "df = df.withColumnRenamed(\"Country\",\"visiting_country\")\n",
    "\n",
    "df = df.withColumn(\"country\",F.explode(F.split(\"visiting_country\",',')))\n",
    "\n",
    "all_countries = df.select(F.explode(F.split(\"Country\", \",\")))\\\n",
    "      .distinct()\\\n",
    "      .rdd.flatMap(lambda x: x)\\\n",
    "      .collect()\n",
    "df = df.withColumn(\"all_country\",F.lit(all_countries))\n",
    "\n",
    "df = df.dropDuplicates(subset=['Name'])\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"NonVisitedCountryArr\",\n",
    "    F.array_except(F.col(\"all_country\"), F.split(F.col(\"visiting_country\"),','))\n",
    ")\n",
    "df = df.drop(\"country\", \"all_country\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"age\", \"Country\"])\n",
    "df = df.withColumnRenamed(\"Country\", \"visiting_country\")\n",
    "\n",
    "all_countries = (\n",
    "    df.select(F.explode(F.split(\"visiting_country\", \",\")))\n",
    "      .distinct()\n",
    "      .rdd.flatMap(lambda x: x)\n",
    "      .collect()\n",
    ")\n",
    "df = df.withColumn(\"all_country\", F.lit(all_countries))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"NonVisitedCountryArr\",\n",
    "    F.array_join(F.array_except(\n",
    "        F.col(\"all_country\"),\n",
    "        F.split(F.col(\"visiting_country\"), \",\")\n",
    "    ),',')\n",
    ")\n",
    "\n",
    "df = df.drop(\"age\",\"all_country\")\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "players_data = [\n",
    "    (\"Sachin-IND\", 18694, \"93/49\"),\n",
    "    (\"Ricky-AUS\", 11274, \"66/31\"),\n",
    "    (\"Lara-WI\", 10222, \"45/21\"),\n",
    "    (\"Rahul-IND\", 10355, \"95/11\"),\n",
    "    (\"Jhonty-SA\", 7051, \"43/5\"),\n",
    "    (\"Hayden-AUS\", 8722, \"67/19\")\n",
    "]\n",
    "\n",
    "players_df = spark.createDataFrame(\n",
    "    players_data,\n",
    "    [\"player\", \"runs\", \"50s/100s\"]\n",
    ")\n",
    "\n",
    "countries_data = [\n",
    "    (\"IND\", \"India\"),\n",
    "    (\"AUS\", \"Australia\"),\n",
    "    (\"WI\", \"WestIndies\"),\n",
    "    (\"SA\", \"SouthAfrica\")\n",
    "]\n",
    "\n",
    "countries_df = spark.createDataFrame(\n",
    "    countries_data,\n",
    "    [\"SRT\", \"country\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ce9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = players_df.withColumn('SRT',F.split(F.col('player'),'-')[1])\n",
    "players_df = players_df.withColumn('player_name',F.split(F.col('player'),'-')[0])\n",
    "players_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a895c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = players_df.withColumn('Sum',F.split('50s/100s','/')[0].cast('int')+F.split('50s/100s','/')[1].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = players_df.join(countries_df,on='SRT',how='inner')\n",
    "# join_df.select('player_name','country','runs','sum').show()\n",
    "join_df.filter(F.col('sum')>90)\\\n",
    "    .select('player_name','country','runs','sum').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca185e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (100, \"IT\", 100, \"2024-05-12\"),\n",
    "    (200, \"IT\", 100, \"2024-06-12\"),\n",
    "    (100, \"FIN\", 400, \"2024-07-12\"),\n",
    "    (300, \"FIN\", 500, \"2024-07-12\"),\n",
    "    (300, \"FIN\", 1543, \"2024-07-12\"),\n",
    "    (300, \"FIN\", 1500, \"2024-07-12\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"empid\", IntegerType(), True),\n",
    "    StructField(\"dept\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df1 =df.groupBy('empid')\\\n",
    "  .count()\\\n",
    "  .filter(F.col(\"count\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3181349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+--------+------------+\n",
      "| id|  numbers|values|new_nums|condtion|convert_list|\n",
      "+---+---------+------+--------+--------+------------+\n",
      "|  A|[1, 2, 3]| 1,2,3|    true|     yes|   [1, 2, 3]|\n",
      "|  B|[1, 2, 3]| 1,2,3|   false|      no|   [1, 2, 3]|\n",
      "|  C|[1, 2, 3]| 1,2,3|   false|      no|   [1, 2, 3]|\n",
      "+---+---------+------+--------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "data = [('A',1),('A',2),('A',3),\n",
    "\t\t('B',1),('B',2),('B',3),\n",
    "\t\t('C',1),('C',2),('C',3)]\n",
    "columns = ['id','number']\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema= columns)\n",
    "\n",
    "df1 = df.groupBy('id').agg(F.collect_list(F.col('number')).alias(\"numbers\"))\\\n",
    "        .withColumn(\"values\",F.array_join(F.col('numbers'),','))\\\n",
    "        .withColumn(\"new_nums\",F.lit(df.id =='A'))\\\n",
    "        .withColumn(\"condtion\",F.when(F.col(\"new_nums\")=='True',\"yes\").otherwise(\"no\"))\\\n",
    "        .withColumn(\"convert_list\",F.split(F.col('values'),','))\n",
    "df1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
