{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6379c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CountryLogic\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Country\"])\n",
    "\n",
    "# 1️⃣ Get all unique countries\n",
    "all_countries = (\n",
    "    df.select(F.explode(F.split(\"Country\", \",\")))\n",
    "      .distinct()\n",
    "      .agg(F.collect_set(\"col\").alias(\"AllCountries\"))\n",
    ")\n",
    "\n",
    "all_countries.show(truncate=False)\n",
    "\n",
    "# 2️⃣ Split visited countries\n",
    "df2 = df.withColumn(\"VisitedCountryArr\", F.split(\"Country\", \",\"))\n",
    "df2.show()\n",
    "\n",
    "# 3️⃣ Cross join to get all countries for comparison\n",
    "df3 = df2.crossJoin(all_countries)\n",
    "df3.show(truncate=False)\n",
    "\n",
    "# 4️⃣ Find non visited countries\n",
    "result = df3.withColumn(\n",
    "    \"NonVisitedCountry\",\n",
    "    F.array_join(\n",
    "        F.array_except(F.col(\"AllCountries\"), F.col(\"VisitedCountryArr\")),\n",
    "        \",\"\n",
    "    )\n",
    ").select(\n",
    "    \"Name\",\n",
    "    F.col(\"Country\").alias(\"VisitedCountry\"),\n",
    "    \"NonVisitedCountry\"\n",
    ")\n",
    "\n",
    "result.show(truncate=False)\n",
    "\n",
    "# result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NoCrossJoin\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Country\"])\n",
    "\n",
    "# 1️⃣ Get all unique countries as a Python list\n",
    "all_countries = (\n",
    "    df.select(F.explode(F.split(\"Country\", \",\")))\n",
    "      .distinct()\n",
    "      .rdd.flatMap(lambda x: x)\n",
    "      .collect()\n",
    ")\n",
    "\n",
    "# 2️⃣ Create visited & non-visited columns\n",
    "result = (\n",
    "    df.withColumn(\"VisitedArr\", F.split(\"Country\", \",\"))\n",
    "      .withColumn(\n",
    "          \"NonVisitedCountry\",\n",
    "          F.array_join(\n",
    "              F.array_except(\n",
    "                  F.array(*[F.lit(c) for c in all_countries]),\n",
    "                  F.col(\"VisitedArr\")\n",
    "              ),\n",
    "              \",\"\n",
    "          )\n",
    "      )\n",
    "      .select(\n",
    "          \"Name\",\n",
    "          F.col(\"Country\").alias(\"VisitedCountry\"),\n",
    "          \"NonVisitedCountry\"\n",
    "      )\n",
    ")\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11943ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, explode, collect_set,\n",
    "    array_except, concat_ws\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    \n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"CountryName\"])\n",
    "df1 = df.withColumn(\"VisitedCountryArr\", split(col(\"CountryName\"), \",\"))\n",
    "df_exploded = df1.withColumn(\"country\", explode(col(\"VisitedCountryArr\")))\n",
    "w = Window.partitionBy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3167789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_all = df_exploded.withColumn(\n",
    "    \"AllCountries\",\n",
    "    collect_set(\"country\").over(w)\n",
    ")\n",
    "df_with_all.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df_with_all.dropDuplicates([\"Name\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\n",
    "    \"NonVisitedCountryArr\",\n",
    "    array_except(col(\"AllCountries\"), col(\"VisitedCountryArr\"))\n",
    ")\n",
    "\n",
    "df2.select('Name','VisitedCountryArr','NonVisitedCountryArr').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee617f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    " \n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "data =[(\"Ram\",30,\"Russia\"),\n",
    "\t   (\"Radha\",31,\"Russia,Norway\"),\n",
    "\t   (\"Kannu\",35,\"Norway,Belgium\")\n",
    "\t   ]\n",
    "df = spark.createDataFrame(data,[\"Name\",\"age\",\"Country\"])\n",
    "\n",
    "df = df.withColumnRenamed(\"Country\",\"visiting_country\")\n",
    "\n",
    "df = df.withColumn(\"country\",F.explode(F.split(\"visiting_country\",',')))\n",
    "\n",
    "all_countries = df.select(F.explode(F.split(\"Country\", \",\")))\\\n",
    "      .distinct()/\n",
    "      .rdd.flatMap(lambda x: x)/\n",
    "      .collect()\n",
    "df = df.withColumn(\"all_country\",F.lit(all_countries))\n",
    "\n",
    "df = df.dropDuplicates(subset=['Name'])\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"NonVisitedCountryArr\",\n",
    "    F.array_except(F.col(\"all_country\"), F.split(F.col(\"visiting_country\"),','))\n",
    ")\n",
    "df = df.drop(\"country\", \"all_country\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Ram\", 30, \"Russia\"),\n",
    "    (\"Radha\", 31, \"Russia,Norway\"),\n",
    "    (\"Kannu\", 35, \"Norway,Belgium\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"age\", \"Country\"])\n",
    "df = df.withColumnRenamed(\"Country\", \"visiting_country\")\n",
    "\n",
    "all_countries = (\n",
    "    df.select(F.explode(F.split(\"visiting_country\", \",\")))\n",
    "      .distinct()\n",
    "      .rdd.flatMap(lambda x: x)\n",
    "      .collect()\n",
    ")\n",
    "df = df.withColumn(\"all_country\", F.lit(all_countries))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"NonVisitedCountryArr\",\n",
    "    F.array_join(F.array_except(\n",
    "        F.col(\"all_country\"),\n",
    "        F.split(F.col(\"visiting_country\"), \",\")\n",
    "    ),',')\n",
    ")\n",
    "\n",
    "df = df.drop(\"age\",\"all_country\")\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "players_data = [\n",
    "    (\"Sachin-IND\", 18694, \"93/49\"),\n",
    "    (\"Ricky-AUS\", 11274, \"66/31\"),\n",
    "    (\"Lara-WI\", 10222, \"45/21\"),\n",
    "    (\"Rahul-IND\", 10355, \"95/11\"),\n",
    "    (\"Jhonty-SA\", 7051, \"43/5\"),\n",
    "    (\"Hayden-AUS\", 8722, \"67/19\")\n",
    "]\n",
    "\n",
    "players_df = spark.createDataFrame(\n",
    "    players_data,\n",
    "    [\"player\", \"runs\", \"50s/100s\"]\n",
    ")\n",
    "\n",
    "countries_data = [\n",
    "    (\"IND\", \"India\"),\n",
    "    (\"AUS\", \"Australia\"),\n",
    "    (\"WI\", \"WestIndies\"),\n",
    "    (\"SA\", \"SouthAfrica\")\n",
    "]\n",
    "\n",
    "countries_df = spark.createDataFrame(\n",
    "    countries_data,\n",
    "    [\"SRT\", \"country\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ce9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = players_df.withColumn('SRT',F.split(F.col('player'),'-')[1])\n",
    "players_df = players_df.withColumn('player_name',F.split(F.col('player'),'-')[0])\n",
    "players_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a895c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = players_df.withColumn('Sum',F.split('50s/100s','/')[0].cast('int')+F.split('50s/100s','/')[1].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = players_df.join(countries_df,on='SRT',how='inner')\n",
    "# join_df.select('player_name','country','runs','sum').show()\n",
    "join_df.filter(F.col('sum')>90)\\\n",
    "    .select('player_name','country','runs','sum').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca185e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (100, \"IT\", 100, \"2024-05-12\"),\n",
    "    (200, \"IT\", 100, \"2024-06-12\"),\n",
    "    (100, \"FIN\", 400, \"2024-07-12\"),\n",
    "    (300, \"FIN\", 500, \"2024-07-12\"),\n",
    "    (300, \"FIN\", 1543, \"2024-07-12\"),\n",
    "    (300, \"FIN\", 1500, \"2024-07-12\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"empid\", IntegerType(), True),\n",
    "    StructField(\"dept\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df1 =df.groupBy('empid')\\\n",
    "  .count()\\\n",
    "  .filter(F.col(\"count\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3181349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "data = [('A',1),('A',2),('A',3),\n",
    "\t\t('B',1),('B',2),('B',3),\n",
    "\t\t('C',1),('C',2),('C',3)]\n",
    "columns = ['id','number']\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema= columns)\n",
    "\n",
    "df1 = df.groupBy('id').agg(F.collect_list(F.col('number')).alias(\"numbers\"))\\\n",
    "        .withColumn(\"values\",F.array_join(F.col('numbers'),','))\\\n",
    "        .withColumn(\"new_nums\",F.lit(df.id =='A'))\\\n",
    "        .withColumn(\"condtion\",F.when(F.col(\"new_nums\")=='True',\"yes\").otherwise(\"no\"))\\\n",
    "        .withColumn(\"convert_list\",F.split(F.col('values'),','))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 5000},\n",
    "    {\"sale_id\": 2, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 7000},\n",
    "    {\"sale_id\": 3, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 6500},\n",
    "    {\"sale_id\": 4, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 8000},\n",
    "    {\"sale_id\": 5, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 4000},\n",
    "    {\"sale_id\": 6, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 9000},\n",
    "    {\"sale_id\": 7, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 7500},\n",
    "    {\"sale_id\": 8, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 6000},\n",
    "    {\"sale_id\": 9, \"region\": \"North\", \"salesperson\": \"Kiran\",  \"amount\": 3000},\n",
    "    {\"sale_id\": 10,\"region\": \"South\", \"salesperson\": \"Deepa\",  \"amount\": 5500}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy('region').orderBy(F.col('amount'))\n",
    "\n",
    "df.withColumn('rank',F.rank().over(win))\\\n",
    "  .withColumn('dense_rank',F.dense_rank().over(win))\\\n",
    "  .withColumn('row_num',F.row_number().over(win)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_based_region(a,b):\n",
    "    if a in (\"North\",\"South\"):\n",
    "        return b+1000\n",
    "    else:\n",
    "        return b+500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa051cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_udf = F.udf(inc_based_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36934de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"added_amount\",my_udf('region','amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c48edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. STOP EXISTING SPARK SESSION\n",
    "# =========================================================\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. IMPORTS\n",
    "# =========================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. CREATE SPARK SESSION (WINDOWS + JDBC FIX)\n",
    "# =========================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Postgres_JDBC_EndToEnd\")\n",
    "    .config(\"spark.jars\", r\"C:\\spark\\postgresql-42.7.3.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", r\"C:\\spark\\postgresql-42.7.3.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\", r\"C:\\spark\\postgresql-42.7.3.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. CREATE SAMPLE DATAFRAME\n",
    "# =========================================================\n",
    "data = [\n",
    "    (1,'U1','2024-01-01',100),\n",
    "    (2,'U1','2024-01-05',150),\n",
    "    (3,'U1','2024-01-10',200),\n",
    "    (4,'U1','2024-01-15',120),\n",
    "    (5,'U1','2024-01-20',180),\n",
    "    (6,'U1','2024-01-25',90),\n",
    "    (7,'U2','2024-01-03',50),\n",
    "    (8,'U2','2024-01-15',70),\n",
    "    (9,'U3','2024-02-01',300),\n",
    "    (10,'U3','2024-02-05',250),\n",
    "    (11,'U3','2024-02-10',200),\n",
    "    (12,'U3','2024-02-15',150),\n",
    "    (13,'U3','2024-02-18',100),\n",
    "    (14,'U3','2024-02-20',120)\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"order_date\", T.StringType(), True),\n",
    "    T.StructField(\"amount\", T.IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "\n",
    "print(\"===== DATAFRAME BEFORE WRITE =====\")\n",
    "df.show()\n",
    "print(\"Row count:\", df.count())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. POSTGRES JDBC CONFIG\n",
    "# =========================================================\n",
    "pg_url = \"jdbc:postgresql://localhost:5432/Sachin?ssl=false\"\n",
    "\n",
    "pg_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"tiger\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. WRITE TO POSTGRES\n",
    "# =========================================================\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", pg_url) \\\n",
    "    .option(\"dbtable\", \"public.orders_int\") \\\n",
    "    .option(\"user\", pg_props[\"user\"]) \\\n",
    "    .option(\"password\", pg_props[\"password\"]) \\\n",
    "    .option(\"driver\", pg_props[\"driver\"]) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"✅ WRITE COMPLETED\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. READ BACK FROM POSTGRES (PROOF)\n",
    "# =========================================================\n",
    "read_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", pg_url) \\\n",
    "    .option(\"dbtable\", \"public.orders_int\") \\\n",
    "    .option(\"user\", pg_props[\"user\"]) \\\n",
    "    .option(\"password\", pg_props[\"password\"]) \\\n",
    "    .option(\"driver\", pg_props[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "print(\"===== DATA READ BACK FROM POSTGRES =====\")\n",
    "read_df.show()\n",
    "print(\"Row count in Postgres:\", read_df.count())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. STOP SPARK\n",
    "# =========================================================\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85d2828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file created successfully at C:/Git files/My git files/PySpark/files/orders_by_user.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS\n",
    "# =========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# =========================================================\n",
    "# 2. SPARK SESSION\n",
    "# =========================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Excel_Multiple_Sheets\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. CREATE SAMPLE DATA\n",
    "# =========================================================\n",
    "data = [\n",
    "    (1,'U1','2024-01-01',100),\n",
    "    (2,'U1','2024-01-05',150),\n",
    "    (3,'U1','2024-01-10',200),\n",
    "    (4,'U1','2024-01-15',120),\n",
    "    (5,'U1','2024-01-20',180),\n",
    "    (6,'U1','2024-01-25',90),\n",
    "    (7,'U2','2024-01-03',50),\n",
    "    (8,'U2','2024-01-15',70),\n",
    "    (9,'U3','2024-02-01',300),\n",
    "    (10,'U3','2024-02-05',250),\n",
    "    (11,'U3','2024-02-10',200),\n",
    "    (12,'U3','2024-02-15',150),\n",
    "    (13,'U3','2024-02-18',100),\n",
    "    (14,'U3','2024-02-20',120)\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"order_date\", T.StringType(), True),\n",
    "    T.StructField(\"amount\", T.IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# =========================================================\n",
    "# 4. TRANSFORMATIONS\n",
    "# =========================================================\n",
    "df = df.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "\n",
    "# =========================================================\n",
    "# 5. WRITE SINGLE EXCEL WITH MULTIPLE SHEETS\n",
    "# =========================================================\n",
    "output_dir = \"C:/Git files/My git files/PySpark/files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_file = f\"{output_dir}/orders_by_user.xlsx\"\n",
    "\n",
    "# Convert Spark DF to Pandas\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Create Excel writer\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    for user_id, user_df in pdf.groupby(\"user_id\"):\n",
    "        user_df.to_excel(\n",
    "            writer,\n",
    "            sheet_name=user_id,   # Sheet name = user_id\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "print(f\"✅ Excel file created successfully at {output_file}\")\n",
    "\n",
    "# =========================================================\n",
    "# 6. STOP SPARK\n",
    "# =========================================================\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"HADOOP_HOME =\", os.environ.get(\"HADOOP_HOME\"))\n",
    "print(\"PATH contains hadoop =\", \"C:\\\\hadoop\\\\bin\" in os.environ.get(\"PATH\", \"\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
