{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import  pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "columns = StructType([\n",
    "            StructField('order_id',IntegerType(),True),\n",
    "            StructField('customer_id',StringType(),True),\n",
    "            StructField('order_date',StringType(),True),\n",
    "            StructField('amount',IntegerType(),True),\n",
    "            StructField('region',StringType(),True),\n",
    "            StructField('product',StringType(),True)\n",
    "            ])\n",
    "data = [\n",
    "        (1, 'C1', '2024-01-01', 500, 'South', 'Laptop'),\n",
    "        (2, 'C1', '2024-01-05', 700, 'South', 'Mobile'),\n",
    "        (3, 'C2', '2024-01-03', 300, 'North', 'Mobile'),\n",
    "        (4, 'C2', '2024-01-10', 900, 'North', 'Laptop'),\n",
    "        (5, 'C3', '2024-01-02', 400, 'South', 'Tablet'),\n",
    "        (6, 'C3', '2024-01-08', 600, 'South', 'Laptop'),\n",
    "        (7, 'C4', '2024-01-04', 200, 'East', 'Mobile'),\n",
    "        (8, 'C4', '2024-01-09', 500, 'East', 'Tablet'),\n",
    "        (9, 'C5', '2024-01-06', 800, 'West', 'Laptop'),\n",
    "        (10, 'C5', '2024-01-12', 1200, 'West', 'Laptop'),\n",
    "        (10, 'C6', '2024-01-12', 1200, 'West', 'Laptop')\n",
    "        ]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "df = spark.createDataFrame(data=data,schema= columns)\n",
    "\n",
    "df = df.withColumn('order_date',F.col('order_date').cast('date'))\n",
    "\n",
    "#Find total sales per customer.\n",
    "df.groupBy('customer_id').agg(F.sum(F.col(\"amount\"))).show()\n",
    "\n",
    "# # #Find total sales per region.\n",
    "df.groupBy('region').agg(F.sum(F.col(\"amount\"))).show()\n",
    "\n",
    "# #Find the highest order amount per region.\n",
    "df.groupBy('region').agg(F.max(F.col(\"amount\"))).show()\n",
    "\n",
    "# # Find the top-spending customer in each region\n",
    "win = Window.partitionBy('region').orderBy(F.col('amount').desc())\n",
    "\n",
    "df.withColumn('highest_per_region',F.dense_rank().over(win))\\\n",
    "   .filter(F.col('highest_per_region')==1)\\\n",
    "   .select('customer_id','amount','region').show()\n",
    "\n",
    "# # Find customers who placed more than 1 order.\n",
    "df.groupBy('customer_id').agg(F.count('*').alias('number_order'))\\\n",
    "  .filter(F.col('number_order')>1)\\\n",
    "  .show()\n",
    "\n",
    "# # For each customer, find the first and last order date.\n",
    "df.groupBy('customer_id')\\\n",
    "    .agg(F.min(F.col(\"order_date\")).alias(\"first_order\"),\\\n",
    "    F.max(F.col(\"order_date\")).alias(\"last_order\"))\\\n",
    "    .show()\n",
    "\n",
    "# Find running total of sales per customer ordered by order_date\n",
    "win_spec = Window.partitionBy('customer_id').orderBy('order_date')\n",
    "df.withColumn('running total',F.sum('amount').over(win_spec))\\\n",
    "    .select('customer_id','order_date','amount','running total')\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "# QðŸ”Ÿ (Very Common Interview Question)\n",
    "# Find customers whose total spending is greater than the average spending of all customers.\n",
    "\n",
    "df1 =df.agg(F.avg('amount').cast('Int').alias(\"avg_amount\"))\n",
    "df2 =df.groupBy('customer_id').agg(F.sum('amount').alias(\"total_spend\"))\n",
    "df2.filter(df2[\"total_spend\"]>df1[\"avg_amount\"]).show()\n",
    "\n",
    "# Q9ï¸âƒ£\n",
    "# Add a column order_category:\n",
    "# High â†’ amount >= 800\n",
    "# Medium â†’ 400â€“799\n",
    "# Low â†’ < 400\n",
    "\n",
    "df.withColumn('order_category',F.when(F.col('amount')>=800,'High')\\\n",
    "                              .when(((F.col('amount')<=799) | (F.col('amount')>=400)),'medium')\\\n",
    "                              .otherwise('low'))\\\n",
    "                              .show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6275531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find customers whose total spend is greater than 800\n",
    "df.groupBy('customer_id').agg(F.sum('amount').alias(\"total_spend\"))\\\n",
    "  .filter(F.col('total_spend')>800)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column order_rank to rank orders per customer by amount (highest first) using window functions.\n",
    "\n",
    "win_speci = Window.partitionBy('customer_id').orderBy(F.col('amount').desc())\n",
    "\n",
    "df.withColumn(\"order_rank\",F.dense_rank().over(win_speci)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"customer_id\") \\\n",
    "  .agg(F.sum(\"amount\").alias(\"total_spend\")) \\\n",
    "  .orderBy(F.col(\"total_spend\").desc()) \\\n",
    "  .limit(1) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"consecutive_days\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"C1\", \"2024-01-01 10:00:00\"),\n",
    "    (2, \"C1\", \"2024-01-02 11:00:00\"),\n",
    "    (3, \"C1\", \"2024-01-03 09:30:00\"),  # âœ… 3 consecutive days\n",
    "\n",
    "    (4, \"C2\", \"2024-01-01 08:00:00\"),\n",
    "    (5, \"C2\", \"2024-01-03 12:00:00\"),\n",
    "    (6, \"C2\", \"2024-01-04 13:00:00\"),  # âŒ not consecutive (missing Jan 2)\n",
    "\n",
    "    (7, \"C3\", \"2024-01-05 10:00:00\"),\n",
    "    (8, \"C3\", \"2024-01-06 10:30:00\"),\n",
    "    (9, \"C3\", \"2024-01-07 11:00:00\"),\n",
    "    (10,\"C3\", \"2024-01-08 09:00:00\")   # âœ… 4 consecutive days\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"order_ts\", F.to_timestamp(\"order_ts\"))\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get distinct order dates per customer\n",
    "df_distinct = df.select(\n",
    "    \"customer_id\",\n",
    "    F.to_date(\"order_ts\").alias(\"order_date\")\n",
    ")\n",
    "# Step 2: assign row_number per customer ordered by date\n",
    "win = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "df_rn = df_distinct.withColumn(\"rn\", F.row_number().over(win))\n",
    "\n",
    "# Step 3: date - rn trick\n",
    "df_group = df_rn.withColumn(\n",
    "    \"grp_date\",\n",
    "    F.date_sub(F.col(\"order_date\"), F.col(\"rn\"))\n",
    ")\n",
    "df_group.show()\n",
    "\n",
    "# Step 4: find streaks of 3 or more days\n",
    "result = df_group.groupBy(\"customer_id\", \"grp_date\") \\\n",
    "    .count() \\\n",
    "    .filter(F.col(\"count\") >= 3) \\\n",
    "    .select(\"customer_id\") \\\n",
    "    .distinct()\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a781af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|txn_id|user_id|\n",
      "+------+-------+\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (1, \"U1\", \"C1\", \"2024-01-01 10:00:00\", 500, \"SUCCESS\"),\n",
    "    (2, \"U1\", \"C1\", \"2024-01-01 10:20:00\", 300, \"SUCCESS\"),\n",
    "    (3, \"U1\", \"C1\", \"2024-01-01 11:00:00\", 200, \"FAILED\"),\n",
    "    (4, \"U2\", \"C2\", \"2024-01-02 09:00:00\", 700, \"SUCCESS\"),\n",
    "    (5, \"U2\", \"C2\", \"2024-01-02 09:25:00\", 400, \"SUCCESS\"),\n",
    "    (6, \"U2\", \"C2\", \"2024-01-02 10:30:00\", 100, \"SUCCESS\"),\n",
    "    (7, \"U3\", \"C3\", \"2024-01-03 12:00:00\", 900, \"FAILED\"),\n",
    "    (8, \"U3\", \"C3\", \"2024-01-03 12:40:00\", 800, \"SUCCESS\"),\n",
    "    (9, \"U3\", \"C3\", \"2024-01-03 13:30:00\", 600, \"SUCCESS\"),\n",
    "    (10,\"U1\", \"C1\", \"2024-01-02 10:00:00\", 1000, \"SUCCESS\")\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    \"txn_id\",\n",
    "    \"user_id\",\n",
    "    \"customer_id\",\n",
    "    \"txn_time\",\n",
    "    \"amount\",\n",
    "    \"status\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df = df.withColumn(\"txn_id\",F.col(\"txn_id\").cast(\"Int\"))\n",
    "df = df.withColumn(\"user_id\",F.col(\"user_id\").cast(\"String\"))\n",
    "df = df.withColumn(\"customer_id\",F.col(\"customer_id\").cast(\"String\"))\n",
    "df = df.withColumn(\"txn_time\",F.to_timestamp(F.col(\"txn_time\")))\n",
    "df = df.withColumn(\"amount\",F.col(\"amount\").cast(\"Int\"))\n",
    "df = df.withColumn(\"status\",F.col(\"status\").cast(\"String\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377c1f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|     U1|\n",
      "|     U2|\n",
      "|     U3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find users who have at least 2 consecutive SUCCESS transactions based on transaction time.\n",
    "\n",
    "# Window per user ordered by transaction time\n",
    "win = Window.partitionBy(\"user_id\").orderBy(\"txn_time\")\n",
    "\n",
    "# Get previous transaction status\n",
    "df2 = df.withColumn(\"prev_status\", F.lag(\"status\").over(win))\n",
    "\n",
    "# Find consecutive SUCCESS transactions\n",
    "result = (\n",
    "    df2\n",
    "    .filter((F.col(\"status\") == \"SUCCESS\") & (F.col(\"prev_status\") == \"SUCCESS\"))\n",
    "    .select(\"user_id\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
