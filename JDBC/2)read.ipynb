{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6871e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== DATA READ BACK FROM POSTGRES =====\n",
      "+--------+-------+----------+------+\n",
      "|order_id|user_id|order_date|amount|\n",
      "+--------+-------+----------+------+\n",
      "|      13|     U3|2024-02-18|   100|\n",
      "|       8|     U2|2024-01-15|    70|\n",
      "|       2|     U1|2024-01-05|   150|\n",
      "|      14|     U3|2024-02-20|   120|\n",
      "|       3|     U1|2024-01-10|   200|\n",
      "|       6|     U1|2024-01-25|    90|\n",
      "|       7|     U2|2024-01-03|    50|\n",
      "|       4|     U1|2024-01-15|   120|\n",
      "|       5|     U1|2024-01-20|   180|\n",
      "|       1|     U1|2024-01-01|   100|\n",
      "|       9|     U3|2024-02-01|   300|\n",
      "|      10|     U3|2024-02-05|   250|\n",
      "|      11|     U3|2024-02-10|   200|\n",
      "|      12|     U3|2024-02-15|   150|\n",
      "+--------+-------+----------+------+\n",
      "\n",
      "Row count in Postgres: 14\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. STOP EXISTING SPARK SESSION\n",
    "# =========================================================\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. IMPORTS\n",
    "# =========================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. CREATE SPARK SESSION (WINDOWS + JDBC FIX)\n",
    "# =========================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Postgres_JDBC_EndToEnd\")\n",
    "    .config(\"spark.jars\", r\"C:\\spark\\postgresql-42.7.3.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. CREATE SAMPLE DATAFRAME\n",
    "# =========================================================\n",
    "data = [\n",
    "    (1,'U1','2024-01-01',100),\n",
    "    (2,'U1','2024-01-05',150),\n",
    "    (3,'U1','2024-01-10',200),\n",
    "    (4,'U1','2024-01-15',120),\n",
    "    (5,'U1','2024-01-20',180),\n",
    "    (6,'U1','2024-01-25',90),\n",
    "    (7,'U2','2024-01-03',50),\n",
    "    (8,'U2','2024-01-15',70),\n",
    "    (9,'U3','2024-02-01',300),\n",
    "    (10,'U3','2024-02-05',250),\n",
    "    (11,'U3','2024-02-10',200),\n",
    "    (12,'U3','2024-02-15',150),\n",
    "    (13,'U3','2024-02-18',100),\n",
    "    (14,'U3','2024-02-20',120)\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"order_date\", T.StringType(), True),\n",
    "    T.StructField(\"amount\", T.IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "\n",
    "# =========================================================\n",
    "# 5. POSTGRES JDBC CONFIG\n",
    "# =========================================================\n",
    "pg_url = \"jdbc:postgresql://localhost:5432/Sachin\"\n",
    "\n",
    "pg_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"tiger\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 7. READ BACK FROM POSTGRES (PROOF)\n",
    "# =========================================================\n",
    "read_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", pg_url) \\\n",
    "    .option(\"dbtable\", \"public.orders_int\") \\\n",
    "    .option(\"user\", pg_props[\"user\"]) \\\n",
    "    .option(\"password\", pg_props[\"password\"]) \\\n",
    "    .option(\"driver\", pg_props[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "print(\"===== DATA READ BACK FROM POSTGRES =====\")\n",
    "read_df.show()\n",
    "print(\"Row count in Postgres:\", read_df.count())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. STOP SPARK\n",
    "# =========================================================\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
